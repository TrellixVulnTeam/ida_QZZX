import abc
from dataclasses import dataclass, field
from typing import Iterable, Tuple, Any, List

import numpy as np
import pyspark.sql.functions as sf
import pyspark.sql.types as st
from petastorm.codecs import ScalarCodec
from petastorm.unischema import Unischema, UnischemaField
from pyspark.sql import DataFrame
from simexp.spark import Field, ConceptMasksUnion, DataGenerator


class AutoRepr:
    def __repr__(self):
        items = ('{}={}'.format(k, v) for k, v in self.__dict__.items())
        return '{}({})'.format(self.__class__.__name__, ', '.join(items))


class Perturber(AutoRepr, abc.ABC):

    @abc.abstractmethod
    def perturb(self, influential_counts: np.ndarray, counts: np.ndarray) \
            -> Iterable[np.ndarray]:
        """
        Takes in two arrays `counts` and `influential_counts` of the same dimension 1xO,
        where O is the number of objects in a classification task.
        `counts` are concept counts on an image, and `influential_counts` represents a subset of these concepts.
        This subset comprises concepts that one deems influential for the classification of this image.

        From this subset the method derives alternative count arrays that by expectation
        all yield the same prediction as `count`.

        :return sequence of count arrays
        """


class LocalPerturber(Perturber):
    """
    Assumes that given "influential concepts" on an image are a locally sufficient condition for its classification,
    i.e., for images that are similar.
    Derives concept counts for "similar" images by dropping all other, "non-influential" concepts from the given image.
    Generates `max_perturbations` random combinations of dropped concepts.
    """

    def __init__(self, max_perturbations: int = 10):
        """
        :param max_perturbations: upper bound for perturbations to generate
        """
        self.max_perturbations = max_perturbations

    def perturb(self, influential_counts: np.ndarray, counts: np.ndarray) \
            -> Iterable[Tuple[np.ndarray, Any]]:
        droppable_counts = counts - influential_counts
        yield from self._perturb_random(counts, droppable_counts)

    def _perturb_random(self, counts, droppable_counts):
        for _ in range(self.max_perturbations):
            perturbed = counts.copy()
            for drop_index in np.flatnonzero(droppable_counts):
                drop_count = np.random.randint(droppable_counts[drop_index] + 1)
                perturbed[drop_index] -= drop_count
            yield perturbed


class InfluenceDetector(AutoRepr, abc.ABC):

    @abc.abstractmethod
    def detect(self, influence_mask: np.ndarray, concept_masks: [np.ndarray]) -> np.ndarray:
        """
        Decides which of the `concept_masks` are influential for a classification
        according to the influential pixels defined by the float matrix `influence_mask`.
        """


class SignInfluenceDetector(InfluenceDetector):
    """
    A concept is considered relevant for the prediction of the classifier
    if the sum of the influence values falling into the mask of the concept is greater than 0.
    """

    def detect(self, influence_mask: np.ndarray, concept_masks: [np.ndarray]) -> np.ndarray:
        influential_masks_idx = []
        for idx, concept_mask in enumerate(concept_masks):
            if np.sum(influence_mask * concept_mask) > 0:
                influential_masks_idx.append(idx)

        return np.asarray(influential_masks_idx, dtype=int)


@dataclass
class PerturbedConceptCountsGenerator(ConceptMasksUnion, DataGenerator):

    # the output schema of this data generator.
    output_schema: Unischema = field(init=False)

    # url of petastorm parquet store of schema `Schema.PIXEL_INFLUENCES`,
    # as generated by the module `observe_influence`
    influences_url: str

    # which detectors to use
    detectors: List[InfluenceDetector]

    # which perturbers to use
    perturbers: List[Perturber]

    def __post_init__(self):
        super().__post_init__()

        influence_fields = [Field.IMAGE_ID, Field.PREDICTED_CLASS, Field.INFLUENCE_ESTIMATOR, Field.PERTURBER,
                            Field.DETECTOR]
        concept_fields = [UnischemaField(concept_name, np.uint8, (), ScalarCodec(st.IntegerType()), False)
                          for concept_name in self.all_concept_names]
        self.output_schema = Unischema('PerturbedConceptCounts', influence_fields + concept_fields)

        id_struct_fields = [st.StructField(name=Field.PERTURBER.name,
                                           dataType=st.StringType(),
                                           nullable=False),
                            st.StructField(name=Field.DETECTOR.name,
                                           dataType=st.StringType(),
                                           nullable=False)]
        concept_struct_fields = [st.StructField(name=concept_name,
                                                dataType=st.IntegerType(),
                                                nullable=False)
                                 for concept_name in self.all_concept_names]
        self._perturb_struct_type = st.ArrayType(st.StructType(id_struct_fields + concept_struct_fields))

    def __getstate__(self):
        # note: we only return the state necessary for the method `_perturb_concepts_on_image`.
        # other attributes will be lost.
        return self.all_concept_names, self.detectors, self.perturbers, self.output_schema

    def __setstate__(self, state):
        (self.all_concept_names, self.detectors, self.perturbers, self.output_schema) = state

    def _get_influences_df(self):
        return self.spark_cfg.session.read.parquet(self.influences_url)

    def to_df(self) -> DataFrame:
        @sf.udf(self._perturb_struct_type)
        def _perturb(concept_names, concept_masks, influence_mask):
            # merge masks and names list from different describers
            concept_names = [name for inner in concept_names for name in inner]
            concept_masks = [mask for masks in concept_masks for mask in Field.CONCEPT_MASKS.decode(masks)]
            influence_mask = Field.INFLUENCE_MASK.decode(influence_mask)

            # generate initial count of each concept on the image
            counts = np.zeros((len(self.all_concept_names, )), dtype=np.uint8)
            for concept_name in concept_names:
                counts[self.all_concept_names.index(concept_name)] += 1

            row_dicts = []

            for detector in self.detectors:
                influential_counts = np.zeros((len(self.all_concept_names),), dtype=np.uint8)
                influential_concepts_idx = detector.detect(influence_mask, concept_masks)
                if len(influential_concepts_idx) > 0:
                    influential_concept_names = np.asarray(concept_names)[influential_concepts_idx]

                    for concept_name, concept_mask in zip(influential_concept_names,
                                                          np.asarray(concept_masks)[influential_concepts_idx]):
                        influential_counts[self.all_concept_names.index(concept_name)] += 1

                for perturber in self.perturbers:
                    for perturbed_counts in perturber.perturb(influential_counts, counts):
                        row_dicts.append({Field.PERTURBER.name: str(perturber),
                                          Field.DETECTOR.name: str(detector),
                                          **dict(zip(self.all_concept_names, perturbed_counts))})

        with self.log_task('Searching influential concepts on images:'):
            return (self.union_df.join(self._get_influences_df(), on=Field.IMAGE_ID.name, how='inner')
                    .withColumn('exploded', sf.explode(_perturb(sf.col(Field.CONCEPT_NAMES.name),
                                                                sf.col(Field.CONCEPT_MASKS.name),
                                                                sf.col(Field.INFLUENCE_MASK.name))))
                    .select(Field.IMAGE_ID.name,
                            Field.PREDICTED_CLASS.name,
                            Field.INFLUENCE_ESTIMATOR.name,
                            sf.col('exploded.*')))
